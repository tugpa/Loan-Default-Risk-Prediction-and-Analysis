{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d297c574",
   "metadata": {},
   "source": [
    "<h2> Predictive Modelling </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd147459",
   "metadata": {},
   "source": [
    "<h3> Importing Libraries and Loading Data </h3>\n",
    "\n",
    "First, we'll import all the necessary libraries for data manipulation, feature engineering, modeling, and evaluation. We'll also load the cleaned and transformed dataset prepared in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39980970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sys\n",
    "from scipy import stats\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80135b6a",
   "metadata": {},
   "source": [
    "<h4> Feature Engineering </h4>\n",
    "\n",
    "In this step, we'll create new, insightful features from the existing data. This process, known as feature engineering, can significantly improve the performance of our predictive models by providing them with more relevant information. We will create features such as:\n",
    "\n",
    "- credit_history_length: The length of the borrower's credit history.\n",
    "- loan_to_income_ratio: The ratio of the loan amount to the borrower's annual income.\n",
    "- instalment_to_income_ratio: The ratio of the loan's monthly installment to the borrower's monthly income.\n",
    "- open_account_ratio : The ratio of open accounts and total accounts per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a217214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_50368\\2377718466.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['open_account_ratio'].fillna(0, inplace=True)\n",
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_50368\\2377718466.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['instalment_to_income_ratio'].fillna(df['instalment_to_income_ratio'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('loan_payments_versions/loan_payments_post_null_imputation.csv')\n",
    "df = df.drop(['id', 'member_id'], axis=1)\n",
    "\n",
    "df['earliest_credit_line'] = pd.to_datetime(df['earliest_credit_line'])\n",
    "df['issue_date'] = pd.to_datetime(df['issue_date'])\n",
    "df['credit_history_length_days'] = (df['issue_date'] - df['earliest_credit_line']).dt.days\n",
    "\n",
    "df['loan_to_income_ratio'] = df['loan_amount'] / df['annual_inc']\n",
    "\n",
    "df['open_account_ratio'] = df['open_accounts'] / df['total_accounts']\n",
    "df['open_account_ratio'].fillna(0, inplace=True)\n",
    "\n",
    "df['instalment_to_income_ratio'] = (\n",
    "    df['instalment'] / (df['annual_inc']/12)\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df['instalment_to_income_ratio'].fillna(df['instalment_to_income_ratio'].median(), inplace=True)\n",
    "\n",
    "df['earliest_credit_line'] = df['earliest_credit_line'].astype(str)\n",
    "df['issue_date'] = df['issue_date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16562470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skew_transformations(df: pd.DataFrame, skew_threshold: float = 0.5, tolerance: float = 0.05, recommend: bool = True, transform: bool = False ):\n",
    "    recommendations = {\n",
    "        'box_cox': [],\n",
    "        'yeo_johnson': [],\n",
    "        'no_transform': []\n",
    "    }\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        original_skew = df[col].skew()\n",
    "\n",
    "        if abs(original_skew) <= skew_threshold:\n",
    "            continue\n",
    "\n",
    "        has_non_positive = (df[col] <= 0).any()\n",
    "        \n",
    "        yj_transformed, _ = stats.yeojohnson(df[col])\n",
    "        skew_yj = pd.Series(yj_transformed).skew()\n",
    "\n",
    "        skew_bc = np.inf\n",
    "        if not has_non_positive:\n",
    "            bc_transformed, _ = stats.boxcox(df[col])\n",
    "            skew_bc = pd.Series(bc_transformed).skew()\n",
    "\n",
    "        best_transform_skew = min(abs(skew_bc), abs(skew_yj))\n",
    "\n",
    "        if best_transform_skew < skew_threshold:\n",
    "            if abs(skew_bc) <= abs(skew_yj) + tolerance:\n",
    "                recommendations['box_cox'].append(col)\n",
    "            else:\n",
    "                recommendations['yeo_johnson'].append(col)\n",
    "        else:\n",
    "            recommendations['no_transform'].append(col)\n",
    "\n",
    "    if recommend == True:\n",
    "        print(\"Columns recommended for Box-Cox transformation:\")\n",
    "        if recommendations['box_cox']:\n",
    "            for col in recommendations['box_cox']:\n",
    "                print(f\"  - {col}\")\n",
    "        else:\n",
    "            print(\"  - None\")\n",
    "\n",
    "        print(\"\\nColumns recommended for Yeo-Johnson transformation:\")\n",
    "        if recommendations['yeo_johnson']:\n",
    "            for col in recommendations['yeo_johnson']:\n",
    "                print(f\"  - {col}\")\n",
    "        else:\n",
    "            print(\"  - None\")\n",
    "\n",
    "        print(\"\\nSkewed columns where no transformation was effective:\")\n",
    "        if recommendations['no_transform']:\n",
    "            for col in recommendations['no_transform']:\n",
    "                print(f\"  - {col}\")\n",
    "        else:\n",
    "            print(\"  - None\")\n",
    "\n",
    "    if transform == True:\n",
    "        for col in recommendations['box_cox']:\n",
    "            df[col], _ = stats.boxcox(df[col])\n",
    "            print(f\"Applied Box-Cox transformation to '{col}'.\")\n",
    "        for col in recommendations['yeo_johnson']:\n",
    "            df[col], _ = stats.yeojohnson(df[col])\n",
    "            print(f\"Applied Yeo-Johnson transformation to '{col}'.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c7b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns recommended for Box-Cox transformation:\n",
      "  - loan_amount\n",
      "  - funded_amount\n",
      "  - instalment\n",
      "  - annual_inc\n",
      "  - open_accounts\n",
      "  - total_accounts\n",
      "  - total_payment\n",
      "  - total_rec_int\n",
      "  - credit_history_length_days\n",
      "  - loan_to_income_ratio\n",
      "  - open_account_ratio\n",
      "  - instalment_to_income_ratio\n",
      "\n",
      "Columns recommended for Yeo-Johnson transformation:\n",
      "  - funded_amount_inv\n",
      "  - inq_last_6mths\n",
      "  - total_payment_inv\n",
      "  - total_rec_prncp\n",
      "  - last_payment_amount\n",
      "\n",
      "Skewed columns where no transformation was effective:\n",
      "  - delinq_2yrs\n",
      "  - out_prncp\n",
      "  - out_prncp_inv\n",
      "  - total_rec_late_fee\n",
      "  - recoveries\n",
      "  - collection_recovery_fee\n",
      "  - collections_12_mths_ex_med\n",
      "Applied Box-Cox transformation to 'loan_amount'.\n",
      "Applied Box-Cox transformation to 'funded_amount'.\n",
      "Applied Box-Cox transformation to 'instalment'.\n",
      "Applied Box-Cox transformation to 'annual_inc'.\n",
      "Applied Box-Cox transformation to 'open_accounts'.\n",
      "Applied Box-Cox transformation to 'total_accounts'.\n",
      "Applied Box-Cox transformation to 'total_payment'.\n",
      "Applied Box-Cox transformation to 'total_rec_int'.\n",
      "Applied Box-Cox transformation to 'credit_history_length_days'.\n",
      "Applied Box-Cox transformation to 'loan_to_income_ratio'.\n",
      "Applied Box-Cox transformation to 'open_account_ratio'.\n",
      "Applied Box-Cox transformation to 'instalment_to_income_ratio'.\n",
      "Applied Yeo-Johnson transformation to 'funded_amount_inv'.\n",
      "Applied Yeo-Johnson transformation to 'inq_last_6mths'.\n",
      "Applied Yeo-Johnson transformation to 'total_payment_inv'.\n",
      "Applied Yeo-Johnson transformation to 'total_rec_prncp'.\n",
      "Applied Yeo-Johnson transformation to 'last_payment_amount'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>funded_amount</th>\n",
       "      <th>funded_amount_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>instalment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>employment_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>...</th>\n",
       "      <th>last_payment_date</th>\n",
       "      <th>last_payment_amount</th>\n",
       "      <th>last_credit_pull_date</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "      <th>credit_history_length_days</th>\n",
       "      <th>loan_to_income_ratio</th>\n",
       "      <th>open_account_ratio</th>\n",
       "      <th>instalment_to_income_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.067206</td>\n",
       "      <td>66.565134</td>\n",
       "      <td>118.808918</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.49</td>\n",
       "      <td>16.250750</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>5 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>4.910558</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.439166</td>\n",
       "      <td>-1.170138</td>\n",
       "      <td>-0.699891</td>\n",
       "      <td>-1.517138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82.085119</td>\n",
       "      <td>80.149885</td>\n",
       "      <td>148.884013</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.99</td>\n",
       "      <td>19.755785</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>9 years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>5.293713</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.192767</td>\n",
       "      <td>-0.975149</td>\n",
       "      <td>-0.636054</td>\n",
       "      <td>-1.396256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.179744</td>\n",
       "      <td>86.049365</td>\n",
       "      <td>162.331735</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.49</td>\n",
       "      <td>21.350927</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>8 years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>7.763081</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.259452</td>\n",
       "      <td>-1.072907</td>\n",
       "      <td>-0.796190</td>\n",
       "      <td>-1.455587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86.087774</td>\n",
       "      <td>84.024811</td>\n",
       "      <td>157.691813</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.31</td>\n",
       "      <td>21.634826</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-06</td>\n",
       "      <td>7.815112</td>\n",
       "      <td>2021-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>5.989336</td>\n",
       "      <td>-0.806591</td>\n",
       "      <td>-0.671795</td>\n",
       "      <td>-1.249544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.087774</td>\n",
       "      <td>84.024811</td>\n",
       "      <td>157.691813</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.03</td>\n",
       "      <td>20.649467</td>\n",
       "      <td>A</td>\n",
       "      <td>A1</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>...</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>5.380851</td>\n",
       "      <td>2022-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.178572</td>\n",
       "      <td>-1.362764</td>\n",
       "      <td>-0.674296</td>\n",
       "      <td>-1.644080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54095</th>\n",
       "      <td>30.698122</td>\n",
       "      <td>30.206701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>16.08</td>\n",
       "      <td>6.929430</td>\n",
       "      <td>F</td>\n",
       "      <td>F2</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>5.507457</td>\n",
       "      <td>2016-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.001760</td>\n",
       "      <td>-1.644617</td>\n",
       "      <td>-0.465160</td>\n",
       "      <td>-1.805182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54096</th>\n",
       "      <td>61.087638</td>\n",
       "      <td>59.792063</td>\n",
       "      <td>33.294103</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.64</td>\n",
       "      <td>14.650246</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>4.712596</td>\n",
       "      <td>2016-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>5.843142</td>\n",
       "      <td>-0.847326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.301705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54097</th>\n",
       "      <td>52.402161</td>\n",
       "      <td>51.353703</td>\n",
       "      <td>56.261077</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.75</td>\n",
       "      <td>12.241126</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>1 year</td>\n",
       "      <td>OWN</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>4.371804</td>\n",
       "      <td>2016-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.001760</td>\n",
       "      <td>-1.498827</td>\n",
       "      <td>-0.272802</td>\n",
       "      <td>-1.725990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54098</th>\n",
       "      <td>59.115307</td>\n",
       "      <td>57.876854</td>\n",
       "      <td>99.876726</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.16</td>\n",
       "      <td>14.434660</td>\n",
       "      <td>C</td>\n",
       "      <td>C3</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>4.057488</td>\n",
       "      <td>2021-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.473221</td>\n",
       "      <td>-0.979261</td>\n",
       "      <td>-0.176267</td>\n",
       "      <td>-1.369149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54099</th>\n",
       "      <td>57.024701</td>\n",
       "      <td>55.846167</td>\n",
       "      <td>68.104346</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.59</td>\n",
       "      <td>13.670635</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-10</td>\n",
       "      <td>5.434922</td>\n",
       "      <td>2016-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>6.065360</td>\n",
       "      <td>-0.847326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.296573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54100 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loan_amount  funded_amount  funded_amount_inv       term  int_rate  \\\n",
       "0        68.067206      66.565134         118.808918  36 months      7.49   \n",
       "1        82.085119      80.149885         148.884013  36 months      6.99   \n",
       "2        88.179744      86.049365         162.331735  36 months      7.49   \n",
       "3        86.087774      84.024811         157.691813  36 months     14.31   \n",
       "4        86.087774      84.024811         157.691813  36 months      6.03   \n",
       "...            ...            ...                ...        ...       ...   \n",
       "54095    30.698122      30.206701           0.000000  36 months     16.08   \n",
       "54096    61.087638      59.792063          33.294103  36 months      9.64   \n",
       "54097    52.402161      51.353703          56.261077  36 months      7.75   \n",
       "54098    59.115307      57.876854          99.876726  36 months     13.16   \n",
       "54099    57.024701      55.846167          68.104346  36 months     10.59   \n",
       "\n",
       "       instalment grade sub_grade employment_length home_ownership  ...  \\\n",
       "0       16.250750     A        A4           5 years       MORTGAGE  ...   \n",
       "1       19.755785     A        A3           9 years           RENT  ...   \n",
       "2       21.350927     A        A4           8 years       MORTGAGE  ...   \n",
       "3       21.634826     C        C4            1 year           RENT  ...   \n",
       "4       20.649467     A        A1         10+ years       MORTGAGE  ...   \n",
       "...           ...   ...       ...               ...            ...  ...   \n",
       "54095    6.929430     F        F2          < 1 year           RENT  ...   \n",
       "54096   14.650246     B        B4            1 year           RENT  ...   \n",
       "54097   12.241126     A        A3            1 year            OWN  ...   \n",
       "54098   14.434660     C        C3          < 1 year           RENT  ...   \n",
       "54099   13.670635     C        C2          < 1 year           RENT  ...   \n",
       "\n",
       "       last_payment_date last_payment_amount last_credit_pull_date  \\\n",
       "0                2022-01            4.910558               2022-01   \n",
       "1                2022-01            5.293713               2022-01   \n",
       "2                2021-10            7.763081               2021-10   \n",
       "3                2021-06            7.815112               2021-06   \n",
       "4                2022-01            5.380851               2022-01   \n",
       "...                  ...                 ...                   ...   \n",
       "54095            2016-03            5.507457               2016-03   \n",
       "54096            2016-12            4.712596               2016-12   \n",
       "54097            2016-09            4.371804               2016-08   \n",
       "54098            2016-10            4.057488               2021-04   \n",
       "54099            2016-10            5.434922               2016-09   \n",
       "\n",
       "      collections_12_mths_ex_med policy_code application_type  \\\n",
       "0                            0.0           1       INDIVIDUAL   \n",
       "1                            0.0           1       INDIVIDUAL   \n",
       "2                            0.0           1       INDIVIDUAL   \n",
       "3                            0.0           1       INDIVIDUAL   \n",
       "4                            0.0           1       INDIVIDUAL   \n",
       "...                          ...         ...              ...   \n",
       "54095                        0.0           1       INDIVIDUAL   \n",
       "54096                        0.0           1       INDIVIDUAL   \n",
       "54097                        0.0           1       INDIVIDUAL   \n",
       "54098                        0.0           1       INDIVIDUAL   \n",
       "54099                        0.0           1       INDIVIDUAL   \n",
       "\n",
       "       credit_history_length_days  loan_to_income_ratio open_account_ratio  \\\n",
       "0                        6.439166             -1.170138          -0.699891   \n",
       "1                        6.192767             -0.975149          -0.636054   \n",
       "2                        6.259452             -1.072907          -0.796190   \n",
       "3                        5.989336             -0.806591          -0.671795   \n",
       "4                        6.178572             -1.362764          -0.674296   \n",
       "...                           ...                   ...                ...   \n",
       "54095                    6.001760             -1.644617          -0.465160   \n",
       "54096                    5.843142             -0.847326           0.000000   \n",
       "54097                    6.001760             -1.498827          -0.272802   \n",
       "54098                    6.473221             -0.979261          -0.176267   \n",
       "54099                    6.065360             -0.847326           0.000000   \n",
       "\n",
       "       instalment_to_income_ratio  \n",
       "0                       -1.517138  \n",
       "1                       -1.396256  \n",
       "2                       -1.455587  \n",
       "3                       -1.249544  \n",
       "4                       -1.644080  \n",
       "...                           ...  \n",
       "54095                   -1.805182  \n",
       "54096                   -1.301705  \n",
       "54097                   -1.725990  \n",
       "54098                   -1.369149  \n",
       "54099                   -1.296573  \n",
       "\n",
       "[54100 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skew_transformations(df,transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fe095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outlier_rows(DataFrame: pd.DataFrame, column_name: str, z_score_threshold: int):\n",
    "\n",
    "    mean = np.mean(DataFrame[column_name]) \n",
    "    std = np.std(DataFrame[column_name]) \n",
    "    z_scores = (DataFrame[column_name] - mean) / std \n",
    "    abs_z_scores = pd.Series(abs(z_scores)) \n",
    "    mask = abs_z_scores < z_score_threshold\n",
    "    DataFrame = DataFrame[mask]         \n",
    "    return DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd14b932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: The DataFrame has 54100 rows.\n",
      "After: The DataFrame has 52903 rows.\n"
     ]
    }
   ],
   "source": [
    "outlier_columns = ['loan_amount', 'funded_amount', 'funded_amount_inv', 'int_rate', 'instalment', 'annual_inc', 'dti', 'open_accounts', 'total_accounts', 'total_payment', 'total_payment_inv', 'total_rec_prncp', 'total_rec_int', 'last_payment_amount']\n",
    "\n",
    "print(f'Before: The DataFrame has {df.shape[0]} rows.') \n",
    "\n",
    "for column in outlier_columns: \n",
    "    df = drop_outlier_rows(df, column, 3) \n",
    "    \n",
    "print(f'After: The DataFrame has {df.shape[0]} rows.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034cfb5",
   "metadata": {},
   "source": [
    "The data has been again adjusted for Skewness and Outliers in the presence of the newly added features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e88844",
   "metadata": {},
   "source": [
    "<h3> Data Preparation for Modeling </h3>\n",
    "\n",
    "Before we can train our models, we need to prepare the data. This involves several key steps:\n",
    "- Defining the Target Variable: <br> We will define our target variable, is_good_loan, which will be a binary indicator (1 for a good loan, 0 for a bad loan). This is the variable our models will learn to predict.\n",
    "- Removal of Leaky/Redundant Columns: <br> We will not consider the columns like total_rec_prncp, total_rec_int etc . These features are considered \"leaky\" because they contain information about the loan's outcome that would not be available at the time of prediction, which can lead to an artificially inflated and unrealistic model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669a48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['term', 'grade', 'sub_grade', 'employment_length', 'home_ownership', 'verification_status', 'issue_date', 'payment_plan', 'purpose', 'earliest_credit_line', 'application_type']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('loan_payments_versions/loan_payments_transformed.csv')\n",
    "df = df.drop(['id', 'member_id'], axis=1)\n",
    "\n",
    "good_loan_statuses = [\n",
    "    \"Fully Paid\",\n",
    "    \"Does not meet the credit policy. Status:Fully Paid\",\n",
    "]\n",
    "bad_loan_statuses = [\n",
    "    \"Charged Off\",\n",
    "    \"Does not meet the credit policy. Status:Charged Off\",\n",
    "]\n",
    "\n",
    "historical_df = df[\n",
    "    df[\"loan_status\"].isin(good_loan_statuses + bad_loan_statuses)\n",
    "].copy()\n",
    "historical_df[\"loan_status\"] = historical_df[\"loan_status\"].apply(\n",
    "    lambda x: 1 if x in good_loan_statuses else 0\n",
    ")\n",
    "leaky_columns = [\n",
    "    'last_payment_date',\n",
    "    'last_payment_amount',\n",
    "    'last_credit_pull_date',\n",
    "    'recoveries',\n",
    "    'collection_recovery_fee',\n",
    "    'total_payment',\n",
    "    'total_rec_prncp',\n",
    "    'total_rec_int',\n",
    "    'total_rec_late_fee',\n",
    "]\n",
    "\n",
    "historical_df = historical_df.drop(columns=leaky_columns)\n",
    "categorical_cols = historical_df.select_dtypes(include='object').columns.tolist()\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ca84b",
   "metadata": {},
   "source": [
    "- One-hot Encoding: <br> Machine learning models require numerical input. We'll convert our categorical features (like purpose, home_ownership etc) into a numerical format using one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707125db",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_df_encoded = pd.get_dummies(historical_df,columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc285c2",
   "metadata": {},
   "source": [
    "- Data Splitting: <br> We will split our dataset into a training set and a testing set. The model will learn from the training set, and we will evaluate its performance on the unseen testing set to ensure it generalizes well.\n",
    "- Feature Scaling: <br> We'll scale our numerical features to ensure they are on a similar scale. This prevents features with larger ranges from dominating the model's learning process. We will use StandardScaler for this.\n",
    "- Dimensionality Reduction with PCA: <br> To reduce the complexity of our data and potentially improve model performance, we'll use Principal Component Analysis (PCA). PCA will transform our features into a smaller set of uncorrelated components while retaining most of the original data's variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8e159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components that explain at least 95% of variance: 633\n"
     ]
    }
   ],
   "source": [
    "X = historical_df_encoded.drop('loan_status', axis=1)\n",
    "Y = historical_df_encoded['loan_status']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Number of components that explain at least 95% of variance: {n_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab56a6",
   "metadata": {},
   "source": [
    "<h3> Modeling Iteration 1: Baseline Models on Imbalanced Data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53edffb",
   "metadata": {},
   "source": [
    "Our first step is to establish a performance baseline. We will train our standard classification models on the raw, imbalanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b60afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training and Evaluating Logistic Regression ---\n",
      "\n",
      "Accuracy Score: 0.8846\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 619 1066]\n",
      " [  72 8101]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.37      0.52      1685\n",
      "           1       0.88      0.99      0.93      8173\n",
      "\n",
      "    accuracy                           0.88      9858\n",
      "   macro avg       0.89      0.68      0.73      9858\n",
      "weighted avg       0.89      0.88      0.86      9858\n",
      "\n",
      "====================================================\n",
      "\n",
      "--- Training and Evaluating Random Forest ---\n",
      "\n",
      "Accuracy Score: 0.8284\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  29 1656]\n",
      " [  36 8137]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.02      0.03      1685\n",
      "           1       0.83      1.00      0.91      8173\n",
      "\n",
      "    accuracy                           0.83      9858\n",
      "   macro avg       0.64      0.51      0.47      9858\n",
      "weighted avg       0.77      0.83      0.76      9858\n",
      "\n",
      "====================================================\n",
      "\n",
      "--- Training and Evaluating LightGBM ---\n",
      "[LightGBM] [Info] Number of positive: 19071, number of negative: 3931\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 23002, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.829102 -> initscore=1.579275\n",
      "[LightGBM] [Info] Start training from score 1.579275\n",
      "\n",
      "Accuracy Score: 0.8341\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 115 1570]\n",
      " [  65 8108]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.07      0.12      1685\n",
      "           1       0.84      0.99      0.91      8173\n",
      "\n",
      "    accuracy                           0.83      9858\n",
      "   macro avg       0.74      0.53      0.52      9858\n",
      "weighted avg       0.80      0.83      0.77      9858\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training and Evaluating {name} ---\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\nAccuracy Score: {accuracy_score(y_test, predictions):.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"====================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f95a8",
   "metadata": {},
   "source": [
    "Loan datasets are typically imbalanced as can be seen here, with far more good loans than bad ones. A naive model can achieve high accuracy simply by always predicting the majority class (\"good loan\"). This step highlighted the problem: we saw high accuracy but very poor recall for the minority class (bad loans), meaning the model fails at its primary goal of identifying risky loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd154e8",
   "metadata": {},
   "source": [
    "<h3> Modeling Iteration 2: Addressing Imbalance with Class Weights </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f9f00c",
   "metadata": {},
   "source": [
    "Our next attempt to improve the model involves using a simple yet effective technique: class weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dca8aa",
   "metadata": {},
   "source": [
    "Many scikit-learn models have a class_weight='balanced' parameter. This technique modifies the loss function, applying a higher penalty to misclassifications of the minority class (bad loans). In essence, it tells the model, \"Pay more attention to getting the bad loans right, even if it means making a few more mistakes on the good loans.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b53b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training and Evaluating Logistic Regression (Balanced) ---\n",
      "\n",
      "Accuracy Score: 0.8842\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1349  336]\n",
      " [ 806 7367]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.70      1685\n",
      "           1       0.96      0.90      0.93      8173\n",
      "\n",
      "    accuracy                           0.88      9858\n",
      "   macro avg       0.79      0.85      0.82      9858\n",
      "weighted avg       0.90      0.88      0.89      9858\n",
      "\n",
      "====================================================\n",
      "\n",
      "--- Training and Evaluating Random Forest (Balanced) ---\n",
      "\n",
      "Accuracy Score: 0.8295\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  12 1673]\n",
      " [   8 8165]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.01      0.01      1685\n",
      "           1       0.83      1.00      0.91      8173\n",
      "\n",
      "    accuracy                           0.83      9858\n",
      "   macro avg       0.71      0.50      0.46      9858\n",
      "weighted avg       0.79      0.83      0.75      9858\n",
      "\n",
      "====================================================\n",
      "\n",
      "--- Training and Evaluating LightGBM (Balanced) ---\n",
      "[LightGBM] [Info] Number of positive: 19071, number of negative: 3931\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 23002, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "\n",
      "Accuracy Score: 0.7382\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 896  789]\n",
      " [1792 6381]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.53      0.41      1685\n",
      "           1       0.89      0.78      0.83      8173\n",
      "\n",
      "    accuracy                           0.74      9858\n",
      "   macro avg       0.61      0.66      0.62      9858\n",
      "weighted avg       0.79      0.74      0.76      9858\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Addressing Class Imbalance because of which the models have a very low recall score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced'),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, class_weight='balanced')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Training and Evaluating {name} (Balanced) ---\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(f\"\\nAccuracy Score: {accuracy_score(y_test, predictions):.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, predictions))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"====================================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7623c",
   "metadata": {},
   "source": [
    "All the models were retrained with this parameter and compare the new classification reports to our baseline. We saw a significant improvement in the recall for bad loans, though overall accuracy slightly decreased. This is a worthwhile trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c22caf8",
   "metadata": {},
   "source": [
    "<h3> Modeling Iteration 3: Comparative Analysis of Resampling Techniques Across Multiple Models </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46119b7",
   "metadata": {},
   "source": [
    "This section implements a systematic experiment to determine the most effective strategy for handling our imbalanced dataset. The goal is to compare how different over-sampling techniques perform when paired with various classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a64232",
   "metadata": {},
   "source": [
    "Resamplers Used: <br>\n",
    "- SMOTE: Creates new minority class samples by interpolating between existing minority class neighbors.\n",
    "- ADASYN : Generates more synthetic samples for minority class instances that are harder to learn, focusing on those near the decision boundary.\n",
    "- SMOTE-Tomek: A hybrid method that first uses SMOTE to create synthetic minority samples and then removes Tomek links (pairs of nearest neighbors from opposite classes) to clean up noise.\n",
    "- SMOTE-ENN: Another hybrid method that first uses SMOTE to oversample the minority class and then uses Edited Nearest Neighbours to remove majority class samples that are misclassified by their neighbors.\n",
    "\n",
    "Models Used: <br>\n",
    "- Logistic Regression: A reliable and interpretable linear model.\n",
    "- Random Forest: A powerful ensemble model based on decision trees.\n",
    "- LightGBM: A highly efficient and often top-performing gradient-boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01de5f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Evaluating with SMOTE ==================\n",
      "Applying SMOTE...\n",
      "Resampling complete.\n",
      "------------------------------\n",
      "--- Training and Evaluating Logistic Regression (with SMOTE) ---\n",
      "\n",
      "Accuracy Score: 0.9066\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1337  348]\n",
      " [ 573 7600]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.79      0.74      1685\n",
      "           1       0.96      0.93      0.94      8173\n",
      "\n",
      "    accuracy                           0.91      9858\n",
      "   macro avg       0.83      0.86      0.84      9858\n",
      "weighted avg       0.91      0.91      0.91      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating Random Forest (with SMOTE) ---\n",
      "\n",
      "Accuracy Score: 0.8039\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 225 1460]\n",
      " [ 473 7700]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.13      0.19      1685\n",
      "           1       0.84      0.94      0.89      8173\n",
      "\n",
      "    accuracy                           0.80      9858\n",
      "   macro avg       0.58      0.54      0.54      9858\n",
      "weighted avg       0.75      0.80      0.77      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating LightGBM (with SMOTE) ---\n",
      "[LightGBM] [Info] Number of positive: 19071, number of negative: 19071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 38142, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Accuracy Score: 0.7697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 656 1029]\n",
      " [1241 6932]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.39      0.37      1685\n",
      "           1       0.87      0.85      0.86      8173\n",
      "\n",
      "    accuracy                           0.77      9858\n",
      "   macro avg       0.61      0.62      0.61      9858\n",
      "weighted avg       0.78      0.77      0.78      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "================== Evaluating with ADASYN ==================\n",
      "Applying ADASYN...\n",
      "Resampling complete.\n",
      "------------------------------\n",
      "--- Training and Evaluating Logistic Regression (with ADASYN) ---\n",
      "\n",
      "Accuracy Score: 0.9087\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1367  318]\n",
      " [ 582 7591]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75      1685\n",
      "           1       0.96      0.93      0.94      8173\n",
      "\n",
      "    accuracy                           0.91      9858\n",
      "   macro avg       0.83      0.87      0.85      9858\n",
      "weighted avg       0.92      0.91      0.91      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating Random Forest (with ADASYN) ---\n",
      "\n",
      "Accuracy Score: 0.7997\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 251 1434]\n",
      " [ 541 7632]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.15      0.20      1685\n",
      "           1       0.84      0.93      0.89      8173\n",
      "\n",
      "    accuracy                           0.80      9858\n",
      "   macro avg       0.58      0.54      0.54      9858\n",
      "weighted avg       0.75      0.80      0.77      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating LightGBM (with ADASYN) ---\n",
      "[LightGBM] [Info] Number of positive: 19071, number of negative: 20110\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073661 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 39181, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.486741 -> initscore=-0.053048\n",
      "[LightGBM] [Info] Start training from score -0.053048\n",
      "\n",
      "Accuracy Score: 0.7634\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 691  994]\n",
      " [1338 6835]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.41      0.37      1685\n",
      "           1       0.87      0.84      0.85      8173\n",
      "\n",
      "    accuracy                           0.76      9858\n",
      "   macro avg       0.61      0.62      0.61      9858\n",
      "weighted avg       0.78      0.76      0.77      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "================== Evaluating with SMOTE-Tomek ==================\n",
      "Applying SMOTE-Tomek...\n",
      "Resampling complete.\n",
      "------------------------------\n",
      "--- Training and Evaluating Logistic Regression (with SMOTE-Tomek) ---\n",
      "\n",
      "Accuracy Score: 0.9192\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1354  331]\n",
      " [ 466 7707]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77      1685\n",
      "           1       0.96      0.94      0.95      8173\n",
      "\n",
      "    accuracy                           0.92      9858\n",
      "   macro avg       0.85      0.87      0.86      9858\n",
      "weighted avg       0.92      0.92      0.92      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating Random Forest (with SMOTE-Tomek) ---\n",
      "\n",
      "Accuracy Score: 0.8079\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 229 1456]\n",
      " [ 438 7735]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.14      0.19      1685\n",
      "           1       0.84      0.95      0.89      8173\n",
      "\n",
      "    accuracy                           0.81      9858\n",
      "   macro avg       0.59      0.54      0.54      9858\n",
      "weighted avg       0.76      0.81      0.77      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating LightGBM (with SMOTE-Tomek) ---\n",
      "[LightGBM] [Info] Number of positive: 19019, number of negative: 19019\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 38038, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Accuracy Score: 0.7671\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 632 1053]\n",
      " [1243 6930]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.38      0.36      1685\n",
      "           1       0.87      0.85      0.86      8173\n",
      "\n",
      "    accuracy                           0.77      9858\n",
      "   macro avg       0.60      0.61      0.61      9858\n",
      "weighted avg       0.78      0.77      0.77      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "================== Evaluating with SMOTE-ENN ==================\n",
      "Applying SMOTE-ENN...\n",
      "Resampling complete.\n",
      "------------------------------\n",
      "--- Training and Evaluating Logistic Regression (with SMOTE-ENN) ---\n",
      "\n",
      "Accuracy Score: 0.6909\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1507  178]\n",
      " [2869 5304]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.89      0.50      1685\n",
      "           1       0.97      0.65      0.78      8173\n",
      "\n",
      "    accuracy                           0.69      9858\n",
      "   macro avg       0.66      0.77      0.64      9858\n",
      "weighted avg       0.86      0.69      0.73      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating Random Forest (with SMOTE-ENN) ---\n",
      "\n",
      "Accuracy Score: 0.4974\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1189  496]\n",
      " [4459 3714]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.71      0.32      1685\n",
      "           1       0.88      0.45      0.60      8173\n",
      "\n",
      "    accuracy                           0.50      9858\n",
      "   macro avg       0.55      0.58      0.46      9858\n",
      "weighted avg       0.77      0.50      0.55      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Training and Evaluating LightGBM (with SMOTE-ENN) ---\n",
      "[LightGBM] [Info] Number of positive: 6364, number of negative: 17942\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 161415\n",
      "[LightGBM] [Info] Number of data points in the train set: 24306, number of used features: 633\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.261828 -> initscore=-1.036487\n",
      "[LightGBM] [Info] Start training from score -1.036487\n",
      "\n",
      "Accuracy Score: 0.5174\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1237  448]\n",
      " [4309 3864]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.73      0.34      1685\n",
      "           1       0.90      0.47      0.62      8173\n",
      "\n",
      "    accuracy                           0.52      9858\n",
      "   macro avg       0.56      0.60      0.48      9858\n",
      "weighted avg       0.78      0.52      0.57      9858\n",
      "\n",
      "----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "\n",
    "samplers = {\n",
    "    'SMOTE':SMOTE(random_state=42),\n",
    "    \"ADASYN\": ADASYN(random_state=42),\n",
    "    \"SMOTE-Tomek\": SMOTETomek(random_state=42),\n",
    "    \"SMOTE-ENN\": SMOTEENN(random_state=42)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    print(f\"================== Evaluating with {sampler_name} ==================\")\n",
    "    \n",
    "    print(f\"Applying {sampler_name}...\")\n",
    "    X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "    print(\"Resampling complete.\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"--- Training and Evaluating {model_name} (with {sampler_name}) ---\")\n",
    "        \n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        print(f\"\\nAccuracy Score: {accuracy_score(y_test, predictions):.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, predictions))\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, predictions))\n",
    "        print(\"----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0929a74",
   "metadata": {},
   "source": [
    "Based on the output, we can draw some clear conclusions about which models and resampling techniques are most effective for your loan prediction task. The key to success here is not just overall accuracy, but the model's ability to correctly identify the minority class (bad loans, labeled as 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a77c88",
   "metadata": {},
   "source": [
    "Based on a comparative analysis, the optimal strategy for predicting loan defaults is using a Logistic Regression model trained on data balanced with the SMOTE-Tomek resampling technique. This combination proved superior by achieving the best F1-score (0.78) for identifying bad loans while maintaining the highest overall accuracy (92%). While various resampling methods were tested, SMOTE-Tomek's hybrid approach of creating synthetic data and cleaning noisy examples provided the cleanest decision boundary for the linear model. In contrast, more complex tree-based models like Random Forest and LightGBM consistently failed to effectively identify the minority class, and aggressive resampling with SMOTE-ENN drastically hurt precision, making the chosen combination the most reliable and balanced solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
